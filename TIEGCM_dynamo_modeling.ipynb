{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical studies with the TIEGCM to characterize tidal effects on the longitudinal patterns of PRE\n",
    "\n",
    "\n",
    "- This code generates a .nc file that is used in the PRE_2024_Paper_Figures.ipynb notebook.\n",
    "- Note: this took several tens of GB of RAM on my machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import apexpy\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import interpolate\n",
    "\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "import time as time_module\n",
    "from IPython.display import display, clear_output\n",
    "def printerase(s):\n",
    "    clear_output(wait=True)\n",
    "    time_module.sleep(0.01)\n",
    "    print(s)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "import dynamo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solar_zenith_angle(t, lat, lon, alt):\n",
    "    '''\n",
    "    Calculate the angle from zenith to the sun at the time and\n",
    "    location specified.\n",
    "    INPUT:\n",
    "        t - python datetime (assumed to be UT)\n",
    "        lat - Latitude, deg\n",
    "        lon - Longitude, deg\n",
    "        alt - Altitude, km\n",
    "    OUTPUT:\n",
    "        sza - Solar zenith angle (deg)\n",
    "    '''\n",
    "    import ephem\n",
    "    import numpy as np\n",
    "    sun = ephem.Sun()\n",
    "    obs = ephem.Observer()\n",
    "    obs.lon = '%f' % (lon)\n",
    "    obs.lat = '%f' % (lat)\n",
    "    obs.date = t.strftime('%Y/%m/%d %H:%M:%S')\n",
    "    obs.pressure = 0. # ignore refraction. This makes a negligible difference.\n",
    "    obs.elevation = 1000*alt # This makes a negligible difference.\n",
    "    sun.compute(obs)\n",
    "    sza = np.pi/2 - float(sun.alt) # radians\n",
    "    return 180/np.pi * sza # degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_TIEGCM_by_slt(dsday):\n",
    "    '''\n",
    "    For a standard TIEGCM file, return a new Dataset, organized by LT instead of geog longitude.\n",
    "    Assumes variable 'lon' and 'time'.\n",
    "    \n",
    "    Use primitive definition of LT for convenience.\n",
    "    \n",
    "    Note this doesn't affect high-latitude potential, since that is in geomag coordinates. So to be\n",
    "    safe it sets it to NaN\n",
    "    '''\n",
    "    sltg = np.arange(0,24, 24/len(dsday.lon)) # Grid of LTs\n",
    "    # For each timestamp, shift data in lon\n",
    "    dss = []\n",
    "    for n in range(len(dsday.time)):\n",
    "        ds = dsday.isel(time=n)\n",
    "        ds['slt'] = np.mod(ds.time.dt.hour + ds.lon/15., 24)\n",
    "        ds = ds.swap_dims({'lon':'slt'}).sortby('slt')\n",
    "        assert (ds['slt']-sltg).std() < 1e-2\n",
    "        ds['slt'] = sltg # Avoid rounding errors by imposing grid \n",
    "        dss.append(ds)\n",
    "    dsday2 = xr.concat(dss, dim='time') # lon reindexed by LT\n",
    "    \n",
    "    \n",
    "    return dsday2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_TIEGCM_by_lon(dsday2):\n",
    "    '''\n",
    "    For a TIEGCM file organized by LT, return a new Dataset, organized by geog lon instead of LT.\n",
    "    Assumes variable 'slt' and 'time'.\n",
    "    \n",
    "    Use primitive definition of lon-LT conversion, for convenience\n",
    "    '''\n",
    "    long = np.arange(-180,180, 360/len(dsday2.slt)) # Grid of LTs\n",
    "    # For each timestamp, shift data in LT\n",
    "    dss = []\n",
    "    for n in range(len(dsday2.time)):\n",
    "        ds = dsday2.isel(time=n)\n",
    "        ds['lon'] = np.mod(180 + 15*(ds.slt - ds.time.dt.hour), 360) - 180.\n",
    "        ds = ds.swap_dims({'slt':'lon'}).sortby('lon')\n",
    "        assert (ds['lon']-long).std() < 1e-2\n",
    "        ds['lon'] = long # Avoid rounding errors by imposing grid \n",
    "        dss.append(ds)\n",
    "    dsday3 = xr.concat(dss, dim='time') # LT reindexed by lon\n",
    "    dsday3 = dsday3.drop('slt') # Make sure this vestige doesn't stick around as a coordinate\n",
    "    return dsday3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_using_TIEGCM_vars(d, ds, hilat='tiegcm', extrap_tiegcm_down=False, ingest_wind=True, ingest_cond=True):\n",
    "    '''\n",
    "    This is like \"solve_using_TIEGCM_vars\" (which I'm omitting from the v2 nb to reduce confusion) \n",
    "    except it doesn't do the compute_FLI & solve steps.\n",
    "    It also makes sure to save the hlb and llb so they can be used in later steps.\n",
    "    \n",
    "    Take an intialized dynamo object \"d\", fill in winds and conductivities using a TIEGCM dataset \"ds\"\n",
    "    (organized the standard way by lon, not by LT). \"ds\" should be a single timestamp.\n",
    "    \n",
    "    Note this modifies \"d\" in place.\n",
    "    \n",
    "    hilat = 'zero': Use zero potential at upper and lower boundary\n",
    "          = 'tiegcm': Use potential from TIEGCM. (Will be taken from N hemis, which could be different\n",
    "                      than S if mlatde_max is too large)\n",
    "                      \n",
    "    extrap_tiegcm_down = True (default): Extrap the grid down to ~82 km (exponentially for conductivity and linearly for winds)\n",
    "                         False: Keep the bottom at ~98 km. Any grid points below this will be zero-order extrapolated. (Not\n",
    "                                recommended, but slightly faster)\n",
    "                   \n",
    "    ingest_wind = whether to use TIEGCM wind. If not, the variable isn't even initialized\n",
    "    ingest_cond = whether to use TIEGCM cond. If not, the variable isn't even initialized\n",
    "                   \n",
    "    Note: I tried to generalize this to allow multiple times at once, but ran into interpolation trouble.\n",
    "          Just stick with looping over single timestamps for now.\n",
    "          \n",
    "    After doing this, you should be able to call compute_FLI and solve.\n",
    "    '''\n",
    "    \n",
    "    # Warn if altitude extrapolation is too much\n",
    "    if d.alt[0] < 95e3 and not extrap_tiegcm_down:\n",
    "        print('WARNING: Minimum alt gridpoint is %.1f km. Are you sure you want extrap_tiegcm_down=False?' % (d.alt[0]/1e3))\n",
    "    \n",
    "\n",
    "    ds['ZGlev'] = ds.ZG.interp(ilev=ds.lev).assign_coords(lev=ds.lev)\n",
    "\n",
    "    ds2 = ds[['SIGMA_HAL','SIGMA_PED','UN','VN','ZGlev']] # Only save certain variables that need to be interpolated\n",
    "    ds2 = ds2.isel(lev=slice(None,-1)) # Top alt is nan so just ignore it.\n",
    "\n",
    "    # Protect for -180/180 crossover in the interpolation. Copy the last lon to the end of the file\n",
    "    ds2x = ds2.isel(lon=0).assign_coords(lon=180.) # masquerade lon = -180 as +180\n",
    "    ds3 = xr.concat((ds2, ds2x), dim='lon')\n",
    "    \n",
    "    # Optional extrapolation down to ~82 km.\n",
    "    if extrap_tiegcm_down:\n",
    "        lev = ds3.lev.values\n",
    "        dlev = np.diff(lev)[0] # 0.25\n",
    "        N = 13 # How many points to extrapolate. Using 13 gets down to ~82 km.\n",
    "        n = np.arange(N,0,-1)\n",
    "        lev2 = np.concatenate((lev[0]-dlev*n, lev))\n",
    "        \n",
    "        # Replace conductivities with their log, so that interpolation is exponential\n",
    "        for v in ['SIGMA_HAL','SIGMA_PED']:\n",
    "            ds3[v] = np.log(ds3[v])\n",
    "\n",
    "        ds3 = ds3.interp(lev=lev2).interpolate_na(dim='lev', fill_value='extrapolate', method='linear')\n",
    "        # Change conductivity back to real value\n",
    "        for v in ['SIGMA_HAL','SIGMA_PED']:\n",
    "            ds3[v] = np.exp(ds3[v])\n",
    "\n",
    "    # Find the \"lev\" coordinates for each dynamo grid\n",
    "    altN = ds3.ZGlev.interp(lat=d.GLATDN, lon=d.GLONDN)/1e2 # m\n",
    "    altS = ds3.ZGlev.interp(lat=d.GLATDS, lon=d.GLONDS)/1e2 # m\n",
    "\n",
    "    diffS = altS - d.alt\n",
    "    diffN = altN - d.alt\n",
    "    topS = (diffS<=0).all(dim='lev') # Alt is above the TIEGCM grid\n",
    "    topN = (diffN<=0).all(dim='lev') # Alt is above the TIEGCM grid\n",
    "    botS = (diffS>=0).all(dim='lev') # Alt is below\n",
    "    botN = (diffN>=0).all(dim='lev') # Alt is below\n",
    "    midS = ~topS & ~botS & ~d.GLATDN.isnull() # Alt is in the middle\n",
    "    midN = ~topN & ~botN & ~d.GLATDN.isnull() # Alt is in the middle\n",
    "    iS0 = diffS.where(diffS<0 & midS).fillna(value=-np.inf).argmax(dim='lev')\n",
    "    iN0 = diffN.where(diffN<0 & midN).fillna(value=-np.inf).argmax(dim='lev')\n",
    "    iS0 = iS0.where(midS, other=0) # Only fill in the \"mid\" points\n",
    "    iN0 = iN0.where(midN, other=0)\n",
    "\n",
    "    levS0 = ds3.lev.isel(lev=iS0)\n",
    "    levN0 = ds3.lev.isel(lev=iN0)\n",
    "    levS1 = ds3.lev.isel(lev=iS0+1)\n",
    "    levN1 = ds3.lev.isel(lev=iN0+1)\n",
    "    altS0 = altS.isel(lev=iS0)\n",
    "    altN0 = altN.isel(lev=iN0)\n",
    "    altS1 = altS.isel(lev=iS0+1)\n",
    "    altN1 = altN.isel(lev=iN0+1)\n",
    "    levS = levS0 + (levS1 - levS0)/(altS1 - altS0)*(d.alt - altS0)\n",
    "    levN = levN0 + (levN1 - levN0)/(altN1 - altN0)*(d.alt - altN0)\n",
    "    # Handle top and bottom cases\n",
    "    levS.values[topS.values] = ds3.lev[-1]\n",
    "    levN.values[topN.values] = ds3.lev[-1]\n",
    "    levS.values[botS.values] = ds3.lev[0] # TODO: Think about extrapolating lower if you want to test electrojet stuff\n",
    "    levN.values[botN.values] = ds3.lev[0]\n",
    "\n",
    "    # Interpolate the TIEGCM variables onto the dynamo grid\n",
    "    dsiN = ds3.interp(lev=levN, lat=d.GLATDN, lon=d.GLONDN)\n",
    "    dsiS = ds3.interp(lev=levS, lat=d.GLATDS, lon=d.GLONDS)\n",
    "\n",
    "    # Ingest wind and conductivity\n",
    "    if ingest_cond:\n",
    "        d['sigPN'] = (['mlat','mlon','alt'], dsiN.SIGMA_PED.values)\n",
    "        d['sigPS'] = (['mlat','mlon','alt'], dsiS.SIGMA_PED.values)\n",
    "        d['sigHN'] = (['mlat','mlon','alt'], dsiN.SIGMA_HAL.values)\n",
    "        d['sigHS'] = (['mlat','mlon','alt'], dsiS.SIGMA_HAL.values)\n",
    "    if ingest_wind:\n",
    "        d['ugN'] = (['mlat','mlon','alt','vec2'], np.stack((dsiN.UN.values, dsiN.VN.values), axis=3)/1e2)\n",
    "        d['ugS'] = (['mlat','mlon','alt','vec2'], np.stack((dsiS.UN.values, dsiS.VN.values), axis=3)/1e2)\n",
    "\n",
    "    # OPTIONS for high-latitude boundary values\n",
    "    if hilat == 'zero':\n",
    "        hlb = 0. # Zero\n",
    "    elif hilat == 'tiegcm':\n",
    "        phi_gcm_N = ds.PHIM2D.sel(mlat= d.mlatde[-1], method='nearest') # Taking closest match. Avoid tolerance, ignore N/S match.\n",
    "        # phi_gcm_S = ds.PHIM2D.sel(mlat=-d.mlatde[-1], method='nearest', tolerance=0.01)\n",
    "        # assert((phi_gcm_N - phi_gcm_S).std() < 1e-2)\n",
    "        hlb = phi_gcm_N.interp(mlon = d.mlonde).values # Interpolate to dynamo grid. Note TIEGCM mlon grid already has ghost cell\n",
    "\n",
    "    if isinstance(hlb,str) and np.array_equal(hlb, 'Kl=0'):\n",
    "        d['hlb']  = (['mlon'], np.nan*np.zeros(d.N))\n",
    "    else:\n",
    "        d['hlb']  = (['mlon'], hlb)\n",
    "    # We are always using the \"no-current\" lower boundary condition\n",
    "    d['llb']  = (['mlon'], np.nan*np.zeros(d.N))\n",
    "\n",
    "\n",
    "    # Slight hack: Re-initalize time variables to be consistent with TIEGCM file\n",
    "    d.attrs['tref'] = pd.to_datetime(ds.time.item())\n",
    "    d['mlt'] = (['mlon'], d.apex_obj.mlon2mlt(d.mlond, d.tref))\n",
    "    _, lon_at_mageq_at_hR = d.apex_obj.convert(0, d.mlond, 'apex', 'geo', height=d.hR/1e3, )\n",
    "    d['slteq'] = (['mlon'], np.array([dynamo.compute_slt(d.tref, lon) for lon in lon_at_mageq_at_hR]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and fill in Dynamo objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hR = 90e3 # reference height for apex coordinates\n",
    "mlatde_max = 55. # Just low/mid-latitude\n",
    "\n",
    "# OPTION 1: Good resolution\n",
    "mlonde_res = 7.5 \n",
    "mlatde_res = 3.5\n",
    "zskip = 1\n",
    "skip = 2 # for TIEGCM ingestion, choose 1 to average all days (watch memory!) Otherwise downsample for speed/testing. skip=2 is recommended (32 GB on my machine)\n",
    "\n",
    "\n",
    "# OPTION 2: Low resolution, just for testing\n",
    "# mlonde_res = 30.0\n",
    "# mlatde_res = 10.\n",
    "# zskip = 5\n",
    "# skip = 80 \n",
    "\n",
    "\n",
    "\n",
    "# Limit the variables to save \n",
    "vars_to_save = ['mlond', 'mlt', 'slteq', \n",
    "                'hlb', 'llb', 'Ed1', 'Ed2', 've1N', 've1S', 've2N', 've2S',\n",
    "                'mlat', 'mlon', 'alt', 'time', 'mlonde', 'Phi', 'mlate', 'mlone', 'mlatde', 'mlatd', 'hAe', 'hA', 'resid_rms',]\n",
    "\n",
    "tday = pd.to_datetime('2022-10-01') # Random day\n",
    "fn = '/disks/data/icon/Repository/Archive/LEVEL.4/TIEGCM/%i/Data/ICON_L4-3_TIEGCM_%s_v02r000.NC' % (tday.year, tday.strftime('%Y-%m-%d'))\n",
    "dsd = xr.open_dataset(fn)\n",
    "\n",
    "# Use the daily-averaged altitude grid as the final altitude grid going forward\n",
    "# sig from TIEGCM defined on the \"lev\" grid.\n",
    "# wind from TIEGCM defined in the \"lev\" grid.\n",
    "# alt from TIEGCM defined on the \"ilev\" grid. So I need to do something similar as the TIEGCM-MIGHTI-sampling code, so I can use regular grid interpolation.\n",
    "#       I want to be sure the altitude grid of the dynamo code matches the \"ilev\" grid from the TIEGCM model, since that is where u and sig are defined.\n",
    "# sig for dynamo defined on midpoint grid.\n",
    "\n",
    "# Interpolate ZG to the \"lev\" coordinates instead of the \"ilev\" coordinates. Winds are defined on \"lev\"\n",
    "dsd['ZGlev'] = dsd.ZG.interp(ilev=dsd.lev).assign_coords(lev=dsd.lev)\n",
    "z = dsd.ZGlev.mean(dim=['time','lat','lon'])/1e2 # m\n",
    "z = z[:-1] # Drop top altitude because it's nan\n",
    "z = z[::zskip] # Downsample for testing\n",
    "\n",
    "d = dynamo.init(z, mlonde_res = mlonde_res, mlatde_res = mlatde_res, mlatde_max = mlatde_max, tref = tday, hR=hR) # Time just affects IGRF I believe.\n",
    "dynamo.get_B_IGRF(d)\n",
    "\n",
    "d_init = dynamo.copy(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-load TIEGCM files to come up with \"seasonal-average\" dataset\n",
    "\n",
    "Useful xr.Datasets that this creates:\n",
    "- **dsday_seasavg: organized by lon, seasonal averages.**\n",
    "- **dsday_seasglonavg: Same, but with geog longitudinal averaging**\n",
    "\n",
    "\n",
    "# NOTE: To make this work on a separate machine you'll have to specify the correct location of the TIEGCM-ICON files on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling for \"seasonal average\" cases.\n",
    "## OPTION 1: What was used in practice. This takes a lot of memory (~30 GB)\n",
    "skip_seas = 10 \n",
    "\n",
    "## OPTION 2: Small number of files, just for testing\n",
    "# skip_seas = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = datetime.now()\n",
    "    \n",
    "# Open a subset of TIEGCM Of all available TIEGCM files, choose ones that match the relevant doy range\n",
    "dates = pd.date_range('2019-12-21','2022-11-10')[::skip_seas]\n",
    "\n",
    "dss = []\n",
    "printerase('Loading TIEGCM %i files....' % (len(dates)))\n",
    "for tday in dates:\n",
    "    # NOTE: Modify the location below as appropriate for your machine.\n",
    "    fn = '/disks/data/icon/Repository/Archive/LEVEL.4/TIEGCM/%i/Data/ICON_L4-3_TIEGCM_%s_v02r000.NC' % (tday.year, tday.strftime('%Y-%m-%d'))\n",
    "    print('\\t%s'%fn)\n",
    "    dsday = xr.open_dataset(fn)\n",
    "    dsday = dsday[['PHIM2D', 'SIGMA_HAL', 'SIGMA_PED', 'UN', 'VN', 'ZG']] # Only keep certain variables\n",
    "\n",
    "    # Reindex by UT hour instead of datetime\n",
    "    uthr = dsday.time.dt.hour\n",
    "    uthr = np.mod(uthr-1, 24)+1 # This just changes the last timestamp (0 UT) to 24, for simplicitly\n",
    "    dsday['uthr'] = uthr\n",
    "    dsday = dsday.swap_dims({'time':'uthr'}).drop('time')\n",
    "\n",
    "    dsday.load() # Is this dumb?\n",
    "\n",
    "    dss.append(dsday)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sum(dss)/len(dss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make it look like a typical TIEGCM file, by reindexing \"uthr\" dim to an arbitrary datetime\n",
    "tbin = pd.to_datetime('2024-01-01') + pd.to_timedelta(range(1,25),unit='h') # 2024 to distinguish from 2023 in run below\n",
    "ds['time'] = (['uthr'], tbin)\n",
    "dsday = ds.swap_dims({'uthr':'time'}).drop('uthr')\n",
    "\n",
    "# Create these for future use:\n",
    "# dsday_seasavg: organized by lon, seasonal averages.\n",
    "# dsday_seasglonavg: Same, but with geog longitudinal averaging\n",
    "\n",
    "# And maybe or maybe not useful: dsday_seas_slt: organized by slt\n",
    "\n",
    "dsday_seasavg = dsday\n",
    "dsday_seasavg_slt = view_TIEGCM_by_slt(dsday_seasavg)\n",
    "dsdayx = dsday_seasavg_slt.copy()\n",
    "for v in dsdayx.keys(): # Loop over data variables\n",
    "\n",
    "    if v == 'ZG': # Skip this; we don't want to mess with coordinate system\n",
    "        continue\n",
    "\n",
    "    if 'slt' in dsdayx[v].dims: # Replace it with the longitude-mean (which here means UT mean)\n",
    "        dsdayx[v] = 0*dsdayx[v] + dsdayx[v].mean(dim='time')\n",
    "\n",
    "dsday_seasglonavg = view_TIEGCM_by_lon(dsdayx)\n",
    "\n",
    "dsdayx.close()\n",
    "del dsdayx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONALLY USE THIS TO SAVE MEMORY\n",
    "for d in dss:\n",
    "    d.close()\n",
    "del dss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all doy bins\n",
    "\n",
    "- This took 14 hours to run on my machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doybe = np.linspace(0,366.,9) # 46d bins to match the data. Edges of bins\n",
    "garbage_collect = True # Whether to delete unused variables, etc., to save memory. Keep as True unless debugging.\n",
    "\n",
    "doyb = (doybe[1:] + doybe[:-1])/2. # middle of bins. Be careful as this is fractional (to keep equal spacing) whereas sometimes you'll want ints.\n",
    "\n",
    "dlist = []\n",
    "tstart = datetime.now()\n",
    "for i in range(len(doyb)):\n",
    "    \n",
    "    doy0 = int(doybe[i])\n",
    "    doy1 = int(doybe[i+1])-1 # Inclusive.\n",
    "    \n",
    "    # Of all available TIEGCM files, choose ones that match the relevant doy range\n",
    "    dates_avail = pd.date_range('2019-12-21','2022-11-10')\n",
    "    doys_avail = dates_avail.dayofyear\n",
    "    dates = dates_avail[(doys_avail >= doy0) & (doys_avail <= doy1)] # inclusive, as above\n",
    "    dates = dates[::skip]\n",
    "\n",
    "    dss = []\n",
    "    printerase('Doys %i-%i Loading TIEGCM %i files....' % (doy0,doy1,len(dates)))\n",
    "    for tday in dates:\n",
    "        \n",
    "        # NOTE: Modify the location below as appropriate for your machine.\n",
    "        fn = '/disks/data/icon/Repository/Archive/LEVEL.4/TIEGCM/%i/Data/ICON_L4-3_TIEGCM_%s_v02r000.NC' % (tday.year, tday.strftime('%Y-%m-%d'))\n",
    "        print('\\t%s'%fn)\n",
    "        dsday = xr.open_dataset(fn)\n",
    "        dsday = dsday[['PHIM2D', 'SIGMA_HAL', 'SIGMA_PED', 'UN', 'VN', 'ZG']] # Only keep certain variables\n",
    "\n",
    "        # Reindex by UT hour instead of datetime\n",
    "        uthr = dsday.time.dt.hour\n",
    "        uthr = np.mod(uthr-1, 24)+1 # This just changes the last timestamp (0 UT) to 24, for simplicitly\n",
    "        dsday['uthr'] = uthr\n",
    "        dsday = dsday.swap_dims({'time':'uthr'}).drop('time')\n",
    "\n",
    "        dsday.load() # Not sure if this is necessary\n",
    "\n",
    "        dss.append(dsday)\n",
    "    gc.collect()\n",
    "\n",
    "    ds = sum(dss)/len(dss)\n",
    "\n",
    "    # Now make it look like a typical TIEGCM file, by reindexing \"uthr\" dim to an arbitrary datetime\n",
    "    tbin = pd.to_datetime('2023-01-01') + pd.to_timedelta(int(doyb[i])-1, unit='day') + pd.to_timedelta(range(1,25),unit='h') # Middle of doy bin\n",
    "    ds['time'] = (['uthr'], tbin) # using 2023 so that I don't mistake it for an actual mission date\n",
    "    dsday = ds.swap_dims({'uthr':'time'}).drop('uthr')\n",
    "\n",
    "    if garbage_collect:\n",
    "        for d in dss:\n",
    "            d.close()\n",
    "        del dss\n",
    "\n",
    "    # Create these for future use:\n",
    "    # dsday_raw: organized by lon\n",
    "    # dsday_glonavg: Same, but with geog longitudinal averaging\n",
    "\n",
    "    # And maybe or maybe not useful: dsday_raw_slt: organized by slt\n",
    "\n",
    "    dsday_raw = dsday\n",
    "    dsday_raw_slt = view_TIEGCM_by_slt(dsday_raw)\n",
    "    dsdayx = dsday_raw_slt.copy()\n",
    "    for v in dsdayx.keys(): # Loop over data variables\n",
    "\n",
    "        if v == 'ZG': # Skip this; we don't want to mess with coordinate system\n",
    "            continue\n",
    "\n",
    "        if 'slt' in dsdayx[v].dims: # Replace it with the longitude-mean (which here means UT mean)\n",
    "            dsdayx[v] = 0*dsdayx[v] + dsdayx[v].mean(dim='time')\n",
    "\n",
    "    dsday_lonavg = view_TIEGCM_by_lon(dsdayx)\n",
    "\n",
    "\n",
    "    d_raws = []\n",
    "    d_lonavgs = []\n",
    "    d_seasavgs = []\n",
    "    d_seasglonavgs = []\n",
    "    for ti in range(len(dsday_raw.time)):\n",
    "        printerase('Doys %i-%i: Ingesting TIEGCM %i' % (doy0,doy1,ti))\n",
    "        t = pd.to_datetime(dsday_raw.time[ti].item())\n",
    "\n",
    "        d = dynamo.copy(d_init)\n",
    "        setup_using_TIEGCM_vars(d, dsday_raw.isel(time=ti))\n",
    "        d = d.assign_coords({'time':t})\n",
    "        d_raws.append(d)\n",
    "\n",
    "        # Glon avg\n",
    "        d = dynamo.copy(d_init)\n",
    "        setup_using_TIEGCM_vars(d, dsday_lonavg.isel(time=ti))\n",
    "        d = d.assign_coords({'time':t})\n",
    "        d_lonavgs.append(d)\n",
    "        \n",
    "        # Seas avg\n",
    "        d = dynamo.copy(d_init)\n",
    "        setup_using_TIEGCM_vars(d, dsday_seasavg.isel(time=ti))\n",
    "        d = d.assign_coords({'time':t})\n",
    "        d_seasavgs.append(d)\n",
    "        \n",
    "        # Both\n",
    "        d = dynamo.copy(d_init)\n",
    "        setup_using_TIEGCM_vars(d, dsday_seasglonavg.isel(time=ti))\n",
    "        d = d.assign_coords({'time':t})\n",
    "        d_seasglonavgs.append(d)\n",
    "\n",
    "    d_raw    = dynamo.concat_over_time(d_raws)\n",
    "    d_glonavg = dynamo.concat_over_time(d_lonavgs)\n",
    "    d_seasavg = dynamo.concat_over_time(d_seasavgs)\n",
    "    d_seasglonavg = dynamo.concat_over_time(d_seasglonavgs)\n",
    "\n",
    "\n",
    "    printerase('Doys %i-%i: Dynamo computations' % (doy0,doy1))\n",
    "\n",
    "\n",
    "    # Setup cases\n",
    "    d_wind_resolve_cond_resolve_hlb_resolve = d_raw # No need to copy\n",
    "\n",
    "\n",
    "    #### Re-do for the different cases (longitude, seasonal averaging)\n",
    "    ####\n",
    "    d = dynamo.copy(d_raw)\n",
    "    d[['ugN','ugS']] = d_glonavg[['ugN','ugS']]\n",
    "    d = d.transpose(*list(d_raw.dims)) # Make sure ordering of dimensions is the same\n",
    "    d_wind_glonavg_cond_resolve_hlb_resolve = d\n",
    "    \n",
    "    #### Including seasonal averaging cases.\n",
    "    ####\n",
    "    d = dynamo.copy(d_raw)\n",
    "    d[['ugN','ugS']] = d_seasavg[['ugN','ugS']]\n",
    "    d = d.transpose(*list(d_raw.dims)) # Make sure ordering of dimensions is the same\n",
    "    d_wind_seasavg_cond_resolve_hlb_resolve = d\n",
    "    \n",
    "    ####\n",
    "    d = dynamo.copy(d_raw)\n",
    "    d[['ugN','ugS']] = d_seasglonavg[['ugN','ugS']]\n",
    "    d = d.transpose(*list(d_raw.dims)) # Make sure ordering of dimensions is the same\n",
    "    d_wind_seasglonavg_cond_resolve_hlb_resolve = d\n",
    "    \n",
    "    \n",
    "\n",
    "    # Solve cases (This takes tens of sec per solve)\n",
    "    d_wind_resolve_cond_resolve_hlb_resolve = dynamo.compute_FLI_solve_over_time(d_wind_resolve_cond_resolve_hlb_resolve)\n",
    "    d_wind_glonavg_cond_resolve_hlb_resolve = dynamo.compute_FLI_solve_over_time(d_wind_glonavg_cond_resolve_hlb_resolve)\n",
    "    d_wind_seasavg_cond_resolve_hlb_resolve = dynamo.compute_FLI_solve_over_time(d_wind_seasavg_cond_resolve_hlb_resolve)\n",
    "    d_wind_seasglonavg_cond_resolve_hlb_resolve = dynamo.compute_FLI_solve_over_time(d_wind_seasglonavg_cond_resolve_hlb_resolve)\n",
    "   \n",
    "    \n",
    "    # For all datasets you want to save, do final stuff.\n",
    "    # 0. Trim the variable list to save space\n",
    "    # 1. Transform to LT (easier to analyze)\n",
    "    # 2. Change \"time\" dimension to \"UT\" for easier concat'ing.\n",
    "    # 3. Create new dim for doy\n",
    "    printerase('Doys %i-%i: Reorganizing and saving' % (doy0,doy1))\n",
    "    \n",
    "    dcases = [d_wind_resolve_cond_resolve_hlb_resolve,\n",
    "              d_wind_glonavg_cond_resolve_hlb_resolve,\n",
    "              d_wind_seasavg_cond_resolve_hlb_resolve,\n",
    "              d_wind_seasglonavg_cond_resolve_hlb_resolve,\n",
    "              ]\n",
    "    tags =   ['wr_cr_hr',\n",
    "              'wg_cr_hr',\n",
    "              'ws_cr_hr',\n",
    "              'wsg_cr_hr',\n",
    "              ]\n",
    "    \n",
    "    # Save all cases over the new \"driver\" dimension\n",
    "    dcase_vec = []\n",
    "    for k in range(len(dcases)):\n",
    "        \n",
    "        dfull = dcases[k]\n",
    "        d = dfull[vars_to_save]\n",
    "        \n",
    "        d = dynamo.view_by_slt(d) \n",
    "\n",
    "        uthr = d.time.dt.hour\n",
    "        uthr = np.mod(uthr-1, 24)+1 # This just changes the last timestamp (0 UT) to 24, for simplicitly\n",
    "        d['ut'] = uthr\n",
    "        d = d.swap_dims({'time':'ut'})\n",
    "        \n",
    "        dcase_vec.append(d.assign_coords({'driver':tags[k]}))\n",
    "        \n",
    "        if garbage_collect:\n",
    "            dfull.close()\n",
    "            del dfull\n",
    "\n",
    "    # Save this case-combined dataset, to be later combined over the \"doy\" dimension\n",
    "    dcase_combined = dynamo.concat_over_drivers(dcase_vec)\n",
    "    dlist.append(dcase_combined.assign_coords({'doy':doyb[i]})) # Note tagging dim with fractional doy\n",
    "            \n",
    "printerase('Almost done... concat...')\n",
    "dall = dynamo.concat_over_doy(dlist)\n",
    "if garbage_collect:\n",
    "    # Save memory\n",
    "    for d in dlist:\n",
    "        d.close()\n",
    "    del dlist\n",
    "    gc.collect()\n",
    "print('Done.')\n",
    "    \n",
    "tstop = datetime.now()\n",
    "dt = (tstop-tstart).total_seconds()/60.\n",
    "print('Total processing time: %.1f min' % (dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'apex_obj' in dall.attrs: # Can't save object attributes as nc\n",
    "    del dall.attrs['apex_obj']\n",
    "dall.to_netcdf('tiegcm_2024_v05.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pre",
   "language": "python",
   "name": "env_pre"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
